{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvxANmfZNhOygTO+7bpf72",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanagbaria/MLcourse/blob/main/Lec8_boosingAlgorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting algorithms\n",
        "Agenda\n",
        "* Gradient Boosting: Regression concept\n",
        "* Gradient Boosting: Regression Calculation\n",
        "* Gradient Boosting: Classification concept\n",
        "* Gradient Boosting: Calculation"
      ],
      "metadata": {
        "id": "qFQtyPayXpzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting for Regression\n",
        "Gradient Boosting Regression is an ensemble technique that builds a strong regressor by combining multiple weak regressors (typically decision trees), each trained to correct the errors of its predecessor.\n",
        "\n",
        "Instead of predicting directly, each new model tries to predict the residuals (errors) of the combined previous models.\n",
        "\n",
        "**Mathematical Form:**\n",
        "Suppose you want to minimize a loss function $L(y, \\hat{y})$:\n",
        "1. Start with an initial model:\n",
        "$F_0(x) = arg \\min_{\\gamma} \\Sigma L(y_i, \\gamma)$\n",
        "\n",
        "2. For each iteration $m = 1, 2, ..., M$:\n",
        "  * Compute pseudo-residuals:\n",
        "  $r_i^m = - [\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F=F_{m-1}}$\n",
        "  * Fit a weak model $h_m(x)$ to $r_i^m$\n",
        "  * Compute step size $\\gamma_m$\n",
        "  *Update: $F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\n",
        "\n",
        "**Common Loss Functions:**\n",
        "* Squared Error: $L = (y - \\hat{y})^2$\n",
        "* Absolute Error:  $L = |y - \\hat{y}|$\n",
        "* Huber Loss: Mix of the above, robust to outliers\n",
        "\n",
        "(Lec8: Slides 3 -- 23)"
      ],
      "metadata": {
        "id": "f1k6bU1YUaQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Fit model\n",
        "reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"R2 Score:\", reg.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Te4aep0jXEfM",
        "outputId": "ed85d53b-2b5b-47fc-87a8-76b84a826002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score: 0.8863388281067902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages:**\n",
        "* Handles non-linear relationships\n",
        "* Customizable with loss functions\n",
        "* Robust to overfitting (if tuned properly)\n"
      ],
      "metadata": {
        "id": "GSdFzTUAXQXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Calculation\n",
        "Let's walk through a step-by-step numerical example of Gradient Boosting Regression using the squared error loss function.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "| $x$ | $y$ |\n",
        "| --- | --- |\n",
        "| 1   | 2   |\n",
        "| 2   | 3   |\n",
        "| 3   | 2.5 |\n",
        "\n",
        "**Step 1: Initial Prediction $F_0(x)$**\n",
        "For squared error, initial prediction is usually the mean of $y$:\n",
        "$F_0(x) =bar{y} = \\frac{2+3+2.5}{3} = 2.5$\n",
        "\n",
        "**Step 2: Compute Residuals (Negative Gradient)**\n",
        "\n",
        "$r_i = y_i - F_0(x_i)$\n",
        "\n",
        "| $x$ | $y$ | $F_0(x)$   | Residual $r$ |\n",
        "| --- | --- | ---------- | ------------ |\n",
        "| 1   | 2   |   2.5      | -0.5         |\n",
        "| 2   | 3   |   2.5      | 0.5          |\n",
        "| 3   | 2.5 |   2.5      | 0.0          |\n",
        "\n",
        "**Step 3: Fit a Simple Tree to Residuals**\n",
        "Letâ€™s say our weak learner is a decision stump:\n",
        "* $x < 1.5 => -0.5$\n",
        "* $x \\ge 1.5 => 0.25$ (average of 0.5 and 0)\n",
        "\n",
        "This is the prediction $h_1(x)$\n",
        "\n",
        "**Step 4: Update Model**\n",
        "With learning rate $\\phi = 1.0$, $F_1(x) = F_0(x) + \\phi h_1(x)$\n",
        "\n",
        "| $x$ | $F_0(x)$ | $h_1(x)$ | $F_1(x)$ |\n",
        "| --- | -------- | -------- | -------- |\n",
        "| 1   | 2.5      | -0.5     | 2.0      |\n",
        "| 2   | 2.5      | 0.25     | 2.75     |\n",
        "| 3   | 2.5      | 0.25     | 2.75     |\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2YxmvOlYTZD"
      }
    }
  ]
}