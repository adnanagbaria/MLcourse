{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4LjMTvQVNqq/cwnQZef9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanagbaria/MLcourse/blob/main/Lec7_classification2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification - part 2\n",
        "Agenda:\n",
        "* Entropy\n",
        "* Mutual Information\n",
        "* Decision Tree Classifier\n",
        "* Model Evaluation"
      ],
      "metadata": {
        "id": "KreyesWWZ_yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy\n",
        "Entropy is a measure from information theory that quantifies the uncertainty or impurity in a set of data. Itâ€™s central in decision trees, classification, and probabilistic models.\n",
        "\n",
        "For a discrete random variable $X$, with classes $c_1, c_2, ..., c_k$:\n",
        "\n",
        "$H(X) = -\\Sigma P(c_i) \\log_2P(c_i)$\n",
        "\n",
        "Where:\n",
        "* $P(c_i)$: probability of class $c_i$\n",
        "* The log base 2 gives the result in bits\n",
        "\n",
        "**Interpretation:**\n",
        "* High entropy = high uncertainty = classes are evenly distributed\n",
        "* Low entropy = low uncertainty = one class dominates\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "| Class Distribution       | Entropy |\n",
        "| ------------------------ | ------- |\n",
        "| \\[0.5, 0.5] (50/50)      | 1.0     |\n",
        "| \\[1.0, 0.0] (pure class) | 0.0     |\n",
        "| \\[0.8, 0.2]              | 0.72    |\n",
        "\n",
        "**In Decision Trees:**\n",
        "Entropy is used to evaluate splits:\n",
        "* The algorithm chooses splits that minimize entropy (maximize information gain).\n",
        "* This leads to more homogeneous child nodes.\n",
        "\n",
        "**In Maximum Entropy Models:**\n",
        "Among all models satisfying the training constraints (e.g., expected feature values), the maximum entropy model is the least biased, making the fewest assumptions beyond what is learned."
      ],
      "metadata": {
        "id": "aHmcp9Dl6fIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give an example on entropy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def entropy(probabilities):\n",
        "    \"\"\"Calculates the entropy for a given set of probabilities.\"\"\"\n",
        "    # Filter out probabilities of 0 to avoid log(0)\n",
        "    probabilities = np.asarray(probabilities)\n",
        "    probabilities = probabilities[probabilities > 0]\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "# Example 1: 50/50 distribution\n",
        "probs1 = [0.5, 0.5]\n",
        "print(f\"Entropy for {probs1}: {entropy(probs1)}\")\n",
        "\n",
        "# Example 2: Pure class\n",
        "probs2 = [1.0, 0.0]\n",
        "print(f\"Entropy for {probs2}: {entropy(probs2)}\")\n",
        "\n",
        "# Example 3: 80/20 distribution\n",
        "probs3 = [0.8, 0.2]\n",
        "print(f\"Entropy for {probs3}: {entropy(probs3)}\")\n",
        "\n",
        "# Example 4: Three classes\n",
        "probs4 = [1/3, 1/3, 1/3]\n",
        "print(f\"Entropy for {probs4}: {entropy(probs4)}\")"
      ],
      "metadata": {
        "id": "HhNq3N-sBe2s",
        "outputId": "0f164889-7e7f-473b-a8a8-f41a65662c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy for [0.5, 0.5]: 1.0\n",
            "Entropy for [1.0, 0.0]: -0.0\n",
            "Entropy for [0.8, 0.2]: 0.7219280948873623\n",
            "Entropy for [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]: 1.584962500721156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mutual Information\n",
        "Mutual Information measures how much knowing one variable reduces uncertainty about another. It quantifies the information shared between two variables.\n",
        "\n",
        "For discrete variables $X$ and $Y$:\n",
        "\n",
        "$I(X;Y) = \\Sigma_{x \\in X} \\Sigma_{y \\in Y} P(x, y) \\log_2(\\frac{P(x,y)}{P(x)P(y)}) $\n",
        "* $P(x,y)$: joint probability\n",
        "* $P(x), P(y)$: marginal probabilities\n",
        "\n",
        "**Interpretation:**\n",
        "* $I(X;Y) = 0$: $X$ and $Y$ are independent\n",
        "* Higher MI = stronger dependence\n",
        "\n",
        "**Relation to Entropy:**\n",
        "\n",
        "$I(X;Y) = H(X) - H(X|Y)$\n",
        "\n",
        "Measures the reduction in uncertainty of $X$ after observing $Y$\n",
        "\n",
        "**Use in Machine Learning:**\n",
        "* Feature selection: Rank features by how much information they share with the target\n",
        "* Decision trees: Sometimes used instead of information gain\n",
        "* Unsupervised learning: Measure association between variables (e.g., clustering labels and true labels)\n"
      ],
      "metadata": {
        "id": "9PrsQQn8IW0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "mi = mutual_info_classif(X, y)\n",
        "print(\"Mutual Information scores:\", mi)\n"
      ],
      "metadata": {
        "id": "NZL8y1NAKFZv",
        "outputId": "3f2a3414-1d18-4e3d-a6f9-b34f8b5a619b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutual Information scores: [0.49844807 0.23616305 0.98698301 0.99202902]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n"
      ],
      "metadata": {
        "id": "J1iQoZQNMaQW"
      }
    }
  ]
}